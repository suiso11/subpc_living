"""
éŸ³å£°å¯¾è©±ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
Phase 3: VAD â†’ STT â†’ LLM (Ollama) â†’ TTS â†’ å†ç”Ÿ
æ”¹å–„: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°TTS (æ–‡å˜ä½ã§åˆæˆãƒ»å†ç”Ÿ)ã€Silero VADå¯¾å¿œ
Phase 10: ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰æ¤œçŸ¥ãƒ¢ãƒ¼ãƒ‰è¿½åŠ 
"""
import sys
import re
import time
import threading
import queue
import numpy as np
from pathlib import Path
from typing import Optional

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.audio.stt import WhisperSTT
from src.audio.tts import KokoroTTS
from src.audio.vad import EnergyVAD, create_vad
from src.audio.audio_io import AudioRecorder, AudioPlayer
from src.audio.wakeword import WakeWordDetector
from src.chat.client import OllamaClient
from src.chat.session import ChatSession
from src.chat.config import ChatConfig
from src.memory.vectorstore import VectorStore
from src.memory.rag import RAGRetriever
from src.vision.context import VisionContext
from src.monitor.context import MonitorContext
from src.persona.profile import UserProfile
from src.persona.summarizer import ConversationSummarizer
from src.persona.preloader import SessionPreloader
from src.persona.proactive import ProactiveEngine


class VoicePipeline:
    """éŸ³å£°å¯¾è©±ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    # çŠ¶æ…‹å®šç¾©
    STATE_IDLE = "idle"
    STATE_WAITING = "waiting"  # ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰å¾…æ©Ÿä¸­
    STATE_LISTENING = "listening"
    STATE_PROCESSING = "processing"
    STATE_SPEAKING = "speaking"

    def __init__(
        self,
        chat_config: Optional[ChatConfig] = None,
        stt_model: str = "small",
        tts_models_dir: str = "models/tts/kokoro",
        tts_voice: str = "jf_alpha",
        vad_type: str = "auto",
        streaming_tts: bool = True,
        enable_rag: bool = True,
        enable_vision: bool = True,
        camera_id: int = 0,
        enable_monitor: bool = True,
        enable_persona: bool = True,
        enable_wakeword: bool = False,
        wakeword_models: Optional[list[str]] = None,
        wakeword_threshold: float = 0.5,
    ):
        # ãƒãƒ£ãƒƒãƒˆè¨­å®š
        self.config = chat_config or ChatConfig.load(PROJECT_ROOT / "config" / "chat_config.json")

        # STT (faster-whisper) â€” Phase 9: device="auto" ã§GPUè‡ªå‹•æ¤œå‡º
        self.stt = WhisperSTT(
            model_size=stt_model if stt_model != "small" else "auto",
            language="ja",
        )

        # TTS (kokoro-onnx)
        self.tts = KokoroTTS(
            models_dir=PROJECT_ROOT / tts_models_dir,
            voice=tts_voice,
        )

        # VAD (auto: Sileroå„ªå…ˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§Energy)
        self.vad_type = vad_type
        self.vad = create_vad(vad_type=vad_type, sample_rate=16000)

        # ã‚ªãƒ¼ãƒ‡ã‚£ã‚ª I/O
        self.recorder = AudioRecorder(sample_rate=16000)
        self.player = AudioPlayer(sample_rate=24000)

        # LLM
        self.llm = OllamaClient(
            base_url=self.config.ollama_base_url,
            model=self.config.model,
        )

        # RAG (Phase 4: é•·æœŸè¨˜æ†¶)
        self.enable_rag = enable_rag
        self.rag = None
        if enable_rag:
            try:
                self.vector_store = VectorStore(
                    persist_dir=str(PROJECT_ROOT / "data" / "vectordb"),
                )
                self.rag = RAGRetriever(vector_store=self.vector_store)
            except Exception as e:
                print(f"âš ï¸  RAGåˆæœŸåŒ–ã‚¹ã‚­ãƒƒãƒ—: {e}")
                self.rag = None

        # Vision (Phase 5: æ˜ åƒå…¥åŠ›)
        self.enable_vision = enable_vision
        self.vision_context: Optional[VisionContext] = None
        if enable_vision:
            try:
                emotion_model = str(PROJECT_ROOT / "models" / "vision" / "emotion-ferplus-8.onnx")
                self.vision_context = VisionContext(
                    camera_id=camera_id,
                    analysis_interval=2.0,
                    emotion_model_path=emotion_model,
                )
            except Exception as e:
                print(f"âš ï¸  VisionåˆæœŸåŒ–ã‚¹ã‚­ãƒƒãƒ—: {e}")
                self.vision_context = None

        # Monitor (Phase 6: PCãƒ­ã‚°åé›†)
        self.enable_monitor = enable_monitor
        self.monitor_context: Optional[MonitorContext] = None
        if enable_monitor:
            try:
                self.monitor_context = MonitorContext(
                    db_path=str(PROJECT_ROOT / "data" / "metrics" / "system_metrics.db"),
                    collect_interval=30.0,
                )
            except Exception as e:
                print(f"âš ï¸  MonitoråˆæœŸåŒ–ã‚¹ã‚­ãƒƒãƒ—: {e}")
                self.monitor_context = None

        # Persona (Phase 7: ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚º)
        self.enable_persona = enable_persona
        self.profile: Optional[UserProfile] = None
        self.summarizer: Optional[ConversationSummarizer] = None
        self.preloader: Optional[SessionPreloader] = None
        self.proactive: Optional[ProactiveEngine] = None
        if enable_persona:
            try:
                self.profile = UserProfile(
                    profile_path=str(PROJECT_ROOT / "data" / "profile" / "user_profile.json"),
                )
                self.profile.load()
                self.summarizer = ConversationSummarizer(
                    summaries_dir=str(PROJECT_ROOT / "data" / "profile" / "summaries"),
                )
                self.preloader = SessionPreloader(
                    profile=self.profile,
                    summarizer=self.summarizer,
                )
                self.proactive = ProactiveEngine(
                    profile=self.profile,
                    check_interval=60.0,
                    monitor_context=self.monitor_context,
                )
            except Exception as e:
                print(f"âš ï¸  PersonaåˆæœŸåŒ–ã‚¹ã‚­ãƒƒãƒ—: {e}")
                self.preloader = None
                self.proactive = None

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³
        self.session = ChatSession(
            system_prompt=self.config.system_prompt,
            max_history_turns=self.config.max_history_turns,
            history_dir=str(PROJECT_ROOT / self.config.history_dir),
            rag=self.rag,
            vision_context=self.vision_context,
            monitor_context=self.monitor_context,
            preloader=self.preloader,
        )

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°TTSè¨­å®š
        self.streaming_tts = streaming_tts
        self._tts_queue: queue.Queue = queue.Queue()

        # ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰ (Phase 10)
        self.enable_wakeword = enable_wakeword
        self.wakeword_detector: Optional[WakeWordDetector] = None
        if enable_wakeword:
            self.wakeword_detector = WakeWordDetector(
                model_names=wakeword_models,
                threshold=wakeword_threshold,
            )

        # çŠ¶æ…‹
        self._state = self.STATE_IDLE
        self._running = False
        self._audio_queue: queue.Queue = queue.Queue()

    # --- æ–‡åˆ†å‰²ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ---
    # æ—¥æœ¬èªã®æ–‡æœ«ãƒ‘ã‚¿ãƒ¼ãƒ³: ã€‚ï¼ï¼Ÿ!? + æ”¹è¡Œ
    _SENTENCE_SPLIT_RE = re.compile(r'(?<=[ã€‚ï¼ï¼Ÿ!?\n])')

    @staticmethod
    def _split_sentences(text: str) -> list[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡å˜ä½ã«åˆ†å‰²ã™ã‚‹"""
        parts = VoicePipeline._SENTENCE_SPLIT_RE.split(text)
        return [p for p in parts if p.strip()]

    @property
    def state(self) -> str:
        return self._state

    def initialize(self) -> bool:
        """
        å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–ã€‚èµ·å‹•æ™‚ã«1å›å‘¼ã¶ã€‚

        Returns:
            æˆåŠŸã—ãŸã‚‰ True
        """
        total_steps = 10 if self.enable_wakeword else 9
        print("=" * 50)
        print(" éŸ³å£°å¯¾è©±ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ åˆæœŸåŒ–")
        print("=" * 50)

        # Ollama æ¥ç¶šãƒã‚§ãƒƒã‚¯
        print(f"\n[1/{total_steps}] Ollama æ¥ç¶šç¢ºèª...")
        if not self.llm.is_available():
            print("âŒ Ollamaã«æ¥ç¶šã§ãã¾ã›ã‚“")
            return False
        print("âœ… Ollama OK")

        # STT ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        print(f"\n[2/{total_steps}] STT ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰...")
        try:
            self.stt.load()
            print("âœ… STT OK")
        except Exception as e:
            print(f"âŒ STT ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            return False

        # TTS ãƒã‚§ãƒƒã‚¯
        print(f"\n[3/{total_steps}] TTS ç¢ºèª...")
        try:
            self.tts.load()
            print("âœ… TTS OK")
        except Exception as e:
            print(f"âŒ TTS ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            return False

        # VAD ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ (Energy VADã®å ´åˆã®ã¿ç’°å¢ƒãƒã‚¤ã‚ºè¨ˆæ¸¬)
        print(f"\n[4/{total_steps}] VAD ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³...")
        vad_name = type(self.vad).__name__
        print(f"  VADæ–¹å¼: {vad_name}")
        try:
            if isinstance(self.vad, EnergyVAD):
                print("  ç’°å¢ƒãƒã‚¤ã‚ºã‚’è¨ˆæ¸¬ä¸­ (2ç§’é–“ã€é™ã‹ã«ã—ã¦ãã ã•ã„)...")
                noise_sample = self.recorder.record(2.0)
                self.vad.calibrate(noise_sample)
            else:
                self.vad.calibrate(np.zeros(16000, dtype=np.float32))
            print("âœ… VAD OK")
        except Exception as e:
            print(f"âš ï¸  VAD ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•— (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤ã‚’ä½¿ç”¨): {e}")

        # RAG (Phase 4)
        if self.enable_rag and self.rag is not None:
            print(f"\n[5/{total_steps}] RAG (é•·æœŸè¨˜æ†¶) åˆæœŸåŒ–...")
            try:
                self.vector_store.initialize()
                stats = self.rag.get_stats()
                print(f"âœ… RAG OK (ä¼šè©±: {stats['conversations']}ä»¶, çŸ¥è­˜: {stats['knowledge']}ä»¶)")
            except Exception as e:
                print(f"âš ï¸  RAG åˆæœŸåŒ–å¤±æ•— (RAGãªã—ã§ç¶šè¡Œ): {e}")
                self.session.rag = None
        else:
            print(f"\n[5/{total_steps}] RAG (é•·æœŸè¨˜æ†¶) ã‚¹ã‚­ãƒƒãƒ—")

        # Vision (Phase 5)
        if self.enable_vision and self.vision_context is not None:
            print(f"\n[6/{total_steps}] Vision (æ˜ åƒå…¥åŠ›) åˆæœŸåŒ–...")
            try:
                if self.vision_context.start():
                    import time
                    time.sleep(1.0)  # ã‚«ãƒ¡ãƒ©å®‰å®šå¾…ã¡
                    status = self.vision_context.get_status()
                    emotion_str = "æœ‰åŠ¹" if status["emotion_detection"] else "é¡”æ¤œå‡ºã®ã¿"
                    print(f"âœ… Vision OK (ã‚«ãƒ¡ãƒ©èµ·å‹•, æ„Ÿæƒ…æ¨å®š: {emotion_str})")
                else:
                    print("âš ï¸  ã‚«ãƒ¡ãƒ©ã‚’é–‹ã‘ã¾ã›ã‚“ (Visionãªã—ã§ç¶šè¡Œ)")
                    self.session.vision_context = None
                    self.vision_context = None
            except Exception as e:
                print(f"âš ï¸  Vision åˆæœŸåŒ–å¤±æ•— (Visionãªã—ã§ç¶šè¡Œ): {e}")
                self.session.vision_context = None
                self.vision_context = None
        else:
            print(f"\n[6/{total_steps}] Vision (æ˜ åƒå…¥åŠ›) ã‚¹ã‚­ãƒƒãƒ—")

        # Monitor (Phase 6)
        if self.enable_monitor and self.monitor_context is not None:
            print(f"\n[7/{total_steps}] Monitor (PCãƒ­ã‚°åé›†) åˆæœŸåŒ–...")
            try:
                if self.monitor_context.start():
                    print("âœ… Monitor OK (ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†é–‹å§‹)")
                else:
                    print("âš ï¸  Monitor èµ·å‹•å¤±æ•— (Monitorãªã—ã§ç¶šè¡Œ)")
                    self.session.monitor_context = None
                    self.monitor_context = None
            except Exception as e:
                print(f"âš ï¸  Monitor åˆæœŸåŒ–å¤±æ•— (Monitorãªã—ã§ç¶šè¡Œ): {e}")
                self.session.monitor_context = None
                self.monitor_context = None
        else:
            print(f"\n[7/{total_steps}] Monitor (PCãƒ­ã‚°åé›†) ã‚¹ã‚­ãƒƒãƒ—")

        # Persona (Phase 7)
        if self.enable_persona and self.preloader is not None:
            print(f"\n[8/{total_steps}] Persona (ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚º) åˆæœŸåŒ–...")
            try:
                profile_name = self.profile.name or "(æœªè¨­å®š)"
                facts_count = len(self.profile.extracted_facts)
                today_count = len(self.profile.get_today_schedule())
                print(f"âœ… Persona OK (åå‰: {profile_name}, æŠ½å‡ºæ¸ˆã¿äº‹å®Ÿ: {facts_count}ä»¶, ä»Šæ—¥ã®äºˆå®š: {today_count}ä»¶)")
            except Exception as e:
                print(f"âš ï¸  Persona åˆæœŸåŒ–å¤±æ•— (Personaãªã—ã§ç¶šè¡Œ): {e}")
                self.session.preloader = None
                self.preloader = None
        else:
            print(f"\n[8/{total_steps}] Persona (ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚º) ã‚¹ã‚­ãƒƒãƒ—")

        # Proactive (Phase 7)
        if self.enable_persona and self.proactive is not None:
            print(f"\n[9/{total_steps}] Proactive (ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ç™ºè©±) åˆæœŸåŒ–...")
            try:
                self.proactive.start(callback=self._on_proactive_trigger)
                print("âœ… Proactive OK (ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ç›£è¦–é–‹å§‹)")
            except Exception as e:
                print(f"âš ï¸  Proactive åˆæœŸåŒ–å¤±æ•— (Proactiveãªã—ã§ç¶šè¡Œ): {e}")
                self.proactive = None
        else:
            print(f"\n[9/{total_steps}] Proactive (ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ç™ºè©±) ã‚¹ã‚­ãƒƒãƒ—")

        # ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰ (Phase 10)
        if self.enable_wakeword and self.wakeword_detector is not None:
            print(f"\n[10/{total_steps}] WakeWord (ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰æ¤œçŸ¥) åˆæœŸåŒ–...")
            try:
                if self.wakeword_detector.load():
                    models = self.wakeword_detector.loaded_models
                    print(f"âœ… WakeWord OK (ãƒ¢ãƒ‡ãƒ«: {', '.join(models)}, é–¾å€¤: {self.wakeword_detector.threshold})")
                else:
                    print("âš ï¸  WakeWord ãƒ­ãƒ¼ãƒ‰å¤±æ•— (ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰ãªã—ã§ç¶šè¡Œ)")
                    self.wakeword_detector = None
                    self.enable_wakeword = False
            except Exception as e:
                print(f"âš ï¸  WakeWord åˆæœŸåŒ–å¤±æ•— (ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰ãªã—ã§ç¶šè¡Œ): {e}")
                self.wakeword_detector = None
                self.enable_wakeword = False
        elif self.enable_wakeword:
            print(f"\n[10/{total_steps}] WakeWord (ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰æ¤œçŸ¥) ã‚¹ã‚­ãƒƒãƒ—")

        print("\n" + "=" * 50)
        print(" âœ… åˆæœŸåŒ–å®Œäº†ï¼")
        print("=" * 50)
        return True

    def _on_proactive_trigger(self, trigger_type: str, message: str) -> None:
        """Proactiveã‚¨ãƒ³ã‚¸ãƒ³ã‹ã‚‰ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯: AIç™ºè©±ã‚’TTSã§å†ç”Ÿ"""
        if not self._running:
            return
        if self._state != self.STATE_IDLE:
            return  # ä¼šè©±ä¸­ã¯å‰²ã‚Šè¾¼ã¾ãªã„
        try:
            print(f"\n\U0001f4ac [Proactive/{trigger_type}] {message}")
            wav_data = self.tts.synthesize(message)
            self.player.play_wav(wav_data, blocking=True)
        except Exception as e:
            print(f"\nâš ï¸  Proactive ç™ºè©±å¤±æ•—: {e}")

    def _summarize_session(self) -> None:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«ä¼šè©±ã‚’è¦ç´„ãƒ»çŸ¥è­˜æŠ½å‡º (Phase 7)"""
        if self.summarizer is None or self.session.turn_count < 2:
            return
        try:
            print("â„¹ï¸  ä¼šè©±ã‚’è¦ç´„ä¸­...")
            result = self.summarizer.process_session_end(
                llm=self.llm,
                messages=self.session.messages,
                session_id=self.session.session_id,
                profile=self.profile,
            )
            if result["summary"]:
                print(f"  è¦ç´„: {result['summary'][:80]}...")
            if result["extracted_facts"]:
                print(f"  æŠ½å‡ºã—ãŸäº‹å®Ÿ: {len(result['extracted_facts'])}ä»¶")
        except Exception as e:
            print(f"âš ï¸  ä¼šè©±è¦ç´„å¤±æ•—: {e}")

    def process_voice_turn(self) -> Optional[str]:
        """
        1ã‚¿ãƒ¼ãƒ³ã®éŸ³å£°å¯¾è©±ã‚’å‡¦ç†ã™ã‚‹:
        éŒ²éŸ³ â†’ STT â†’ LLM â†’ TTS â†’ å†ç”Ÿ

        streaming_tts=True ã®å ´åˆã€LLMã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’æ–‡å˜ä½ã§
        é€æ¬¡TTSåˆæˆãƒ»å†ç”Ÿã™ã‚‹ï¼ˆå…¨æ–‡å®Œæˆã‚’å¾…ãŸãªã„ï¼‰ã€‚

        Returns:
            AIã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã€‚ã‚¨ãƒ©ãƒ¼æ™‚ã¯ Noneã€‚
        """
        # --- ãƒªã‚¹ãƒ‹ãƒ³ã‚° ---
        self._state = self.STATE_LISTENING
        print("\nğŸ¤ èã„ã¦ã„ã¾ã™... (è©±ã—çµ‚ã‚ã£ãŸã‚‰è‡ªå‹•æ¤œå‡ºã—ã¾ã™)")

        speech_audio = self._listen_for_speech()
        if speech_audio is None or len(speech_audio) < self.vad.sample_rate * 0.3:
            return None

        # --- STT ---
        self._state = self.STATE_PROCESSING
        print("\nğŸ”„ éŸ³å£°èªè­˜ä¸­...")
        user_text = self.stt.transcribe(speech_audio)

        if not user_text:
            print("  (éŸ³å£°ã‚’èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ)")
            return None

        print(f"\nğŸ‘¤ ã‚ãªãŸ: {user_text}")

        # --- LLM â†’ TTS (ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°) ---
        print("\nğŸ¤– è€ƒãˆä¸­...")
        self.session.add_user_message(user_text)
        messages = self.session.build_messages()

        try:
            if self.streaming_tts:
                response_text = self._stream_llm_with_tts(messages)
            else:
                response_text = self._sequential_llm_then_tts(messages)

            if not response_text:
                return None

            self.session.add_assistant_message(response_text)

        except Exception as e:
            print(f"\nâŒ LLM ã‚¨ãƒ©ãƒ¼: {e}")
            if self.session._messages and self.session._messages[-1]["role"] == "user":
                self.session._messages.pop()
            return None

        self._state = self.STATE_IDLE
        return response_text

    def _stream_llm_with_tts(self, messages: list[dict]) -> str:
        """
        LLMã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’æ–‡å˜ä½ã§TTSåˆæˆãƒ»å†ç”Ÿã™ã‚‹

        LLMãŒãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹é–“ã€æ–‡ã®åŒºåˆ‡ã‚Šã‚’æ¤œå‡ºã—ã¦
        å®Œæˆã—ãŸæ–‡ã‹ã‚‰é †ã«TTSã‚­ãƒ¥ãƒ¼ã«æŠ•å…¥ãƒ»å†ç”Ÿã™ã‚‹ã€‚
        """
        response_text = ""
        sentence_buffer = ""
        tts_thread = None
        played_sentences: list[str] = []

        # TTSå†ç”Ÿãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰
        self._tts_stop = False
        tts_thread = threading.Thread(target=self._tts_worker, daemon=True)
        tts_thread.start()

        self._state = self.STATE_PROCESSING
        try:
            for token in self.llm.generate_stream(
                messages,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                top_k=self.config.top_k,
                num_ctx=self.config.num_ctx,
                repeat_penalty=self.config.repeat_penalty,
            ):
                response_text += token
                sentence_buffer += token
                print(token, end="", flush=True)

                # æ–‡ã®åŒºåˆ‡ã‚Šã‚’ãƒã‚§ãƒƒã‚¯
                sentences = self._split_sentences(sentence_buffer)
                if len(sentences) > 1:
                    # æœ€å¾Œã®è¦ç´ ã¯ã¾ã ä¸å®Œå…¨ãªå¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ä¿æŒ
                    for sent in sentences[:-1]:
                        sent = sent.strip()
                        if sent:
                            self._tts_queue.put(sent)
                            played_sentences.append(sent)
                    sentence_buffer = sentences[-1]

            # æ®‹ã‚Šã®ãƒãƒƒãƒ•ã‚¡ã‚‚é€ä¿¡
            if sentence_buffer.strip():
                self._tts_queue.put(sentence_buffer.strip())
                played_sentences.append(sentence_buffer.strip())

            print()  # æ”¹è¡Œ

        finally:
            # TTSå®Œäº†ã‚’å¾…æ©Ÿ
            self._tts_queue.put(None)  # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
            if tts_thread:
                tts_thread.join(timeout=60)

        return response_text

    def _tts_worker(self) -> None:
        """TTSåˆæˆãƒ»å†ç”Ÿã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰"""
        while True:
            text = self._tts_queue.get()
            if text is None:
                break
            if self._tts_stop:
                break

            self._state = self.STATE_SPEAKING
            try:
                wav_data = self.tts.synthesize(text)
                self.player.play_wav(wav_data, blocking=True)
            except Exception as e:
                print(f"\nâš ï¸  TTSå†ç”Ÿã‚¨ãƒ©ãƒ¼: {e}")

    def _sequential_llm_then_tts(self, messages: list[dict]) -> str:
        """å¾“æ¥ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«æ–¹å¼: LLMå…¨æ–‡å®Œäº†å¾Œã«TTS"""
        response_text = ""
        for token in self.llm.generate_stream(
            messages,
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            top_k=self.config.top_k,
            num_ctx=self.config.num_ctx,
            repeat_penalty=self.config.repeat_penalty,
        ):
            response_text += token
            print(token, end="", flush=True)
        print()

        if not response_text:
            return ""

        # --- TTS & å†ç”Ÿ ---
        self._state = self.STATE_SPEAKING
        print("\nğŸ”Š èª­ã¿ä¸Šã’ä¸­...")
        try:
            wav_data = self.tts.synthesize(response_text)
            self.player.play_wav(wav_data, blocking=True)
        except Exception as e:
            print(f"âš ï¸  TTS/å†ç”Ÿã‚¨ãƒ©ãƒ¼: {e}")

        return response_text

    def _listen_for_speech(self, max_duration: float = 30.0) -> Optional[np.ndarray]:
        """
        VADã‚’ä½¿ã£ã¦ç™ºè©±åŒºé–“ã‚’æ¤œå‡ºã—ã€éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™

        Args:
            max_duration: æœ€å¤§éŒ²éŸ³æ™‚é–“ (ç§’)

        Returns:
            æ¤œå‡ºã•ã‚ŒãŸç™ºè©±ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã€‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã¯ Noneã€‚
        """
        self.vad.reset()
        result_audio = None
        start_time = time.time()

        def audio_callback(indata, frames, time_info, status):
            nonlocal result_audio
            if status:
                pass  # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ç­‰ã¯ç„¡è¦–
            frame = indata[:, 0].copy()  # ãƒ¢ãƒãƒ©ãƒ«ã«
            speech = self.vad.process_frame(frame)
            if speech is not None:
                self._audio_queue.put(speech)

        stream = self.recorder.open_stream(
            callback=audio_callback,
            frame_size=self.vad.frame_size,
        )

        with stream:
            while True:
                try:
                    # ç™ºè©±æ¤œå‡ºå¾…ã¡
                    speech = self._audio_queue.get(timeout=1.0)
                    return speech
                except queue.Empty:
                    elapsed = time.time() - start_time
                    if elapsed > max_duration:
                        print("  (ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ)")
                        return None
                    if self.vad.is_speaking:
                        # ç™ºè©±ä¸­ã®ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿
                        print(".", end="", flush=True)

    def _wait_for_wakeword(self) -> Optional[str]:
        """
        ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰ãŒæ¤œçŸ¥ã•ã‚Œã‚‹ã¾ã§ãƒã‚¤ã‚¯ã‚’ç›£è¦–ã™ã‚‹

        Returns:
            æ¤œçŸ¥ã•ã‚ŒãŸã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰åã€‚ä¸­æ–­æ™‚ã¯ Noneã€‚
        """
        if self.wakeword_detector is None:
            return None

        self._state = self.STATE_WAITING
        self.wakeword_detector.reset()
        detected_word = None

        # ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µã‚¤ã‚º (80ms @ 16kHz = 1280 samples)
        ww_frame_size = self.wakeword_detector.frame_size

        def audio_callback(indata, frames, time_info, status):
            nonlocal detected_word
            if status:
                pass
            if detected_word is not None:
                return  # æ—¢ã«æ¤œçŸ¥æ¸ˆã¿
            frame = indata[:, 0].copy()
            result = self.wakeword_detector.process_frame(frame)
            if result is not None:
                detected_word = result

        stream = self.recorder.open_stream(
            callback=audio_callback,
            frame_size=ww_frame_size,
        )

        with stream:
            while self._running and detected_word is None:
                try:
                    import time
                    time.sleep(0.05)  # 50ms ãƒãƒ¼ãƒªãƒ³ã‚°
                except KeyboardInterrupt:
                    raise

        return detected_word

    def run_interactive(self) -> None:
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–éŸ³å£°å¯¾è©±ãƒ«ãƒ¼ãƒ—"""
        self._running = True
        print("\n" + "=" * 50)

        if self.enable_wakeword and self.wakeword_detector is not None:
            models = self.wakeword_detector.loaded_models
            print(f" ğŸ™ï¸  ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰")
            print(f"  ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰: {', '.join(models)}")
            print(f"  é–¾å€¤: {self.wakeword_detector.threshold}")
        else:
            print(" ğŸ™ï¸  éŸ³å£°å¯¾è©±ãƒ¢ãƒ¼ãƒ‰")
        print("  Ctrl+C ã§çµ‚äº†")
        print("=" * 50)

        try:
            while self._running:
                if self.enable_wakeword and self.wakeword_detector is not None:
                    # ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰: æ¤œçŸ¥ã¾ã§å¾…æ©Ÿ
                    print("\nğŸ‘‚ ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰å¾…æ©Ÿä¸­...")
                    detected = self._wait_for_wakeword()
                    if detected is None:
                        continue
                    print(f"\nâœ¨ ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰æ¤œçŸ¥: {detected}")

                # å¯¾è©±ã‚¿ãƒ¼ãƒ³ã‚’å‡¦ç†
                self.process_voice_turn()

                # Proactive: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£é€šçŸ¥
                if self.proactive is not None:
                    self.proactive.notify_user_activity()
        except KeyboardInterrupt:
            print("\n\nçµ‚äº†ã—ã¾ã™...")
            self._running = False
            # Phase 7: ã‚»ãƒƒã‚·ãƒ§ãƒ³è¦ç´„
            self._summarize_session()
            if self.session.turn_count > 0:
                saved = self.session.save()
                print(f"ä¼šè©±ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {saved}")
            self.llm.close()

    def cleanup(self) -> None:
        """ãƒªã‚½ãƒ¼ã‚¹ã®è§£æ”¾"""
        self._running = False
        if self.wakeword_detector is not None:
            self.wakeword_detector.cleanup()
        if self.proactive is not None:
            self.proactive.stop()
        if self.monitor_context is not None:
            self.monitor_context.stop()
        if self.vision_context is not None:
            self.vision_context.stop()
        self.llm.close()
