"""
éŸ³å£°å¯¾è©±ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
Phase 3: VAD â†’ STT â†’ LLM (Ollama) â†’ TTS â†’ å†ç”Ÿ
æ”¹å–„: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°TTS (æ–‡å˜ä½ã§åˆæˆãƒ»å†ç”Ÿ)ã€Silero VADå¯¾å¿œ
"""
import sys
import re
import time
import threading
import queue
import numpy as np
from pathlib import Path
from typing import Optional

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.audio.stt import WhisperSTT
from src.audio.tts import KokoroTTS
from src.audio.vad import EnergyVAD, create_vad
from src.audio.audio_io import AudioRecorder, AudioPlayer
from src.chat.client import OllamaClient
from src.chat.session import ChatSession
from src.chat.config import ChatConfig
from src.memory.vectorstore import VectorStore
from src.memory.rag import RAGRetriever
from src.vision.context import VisionContext


class VoicePipeline:
    """éŸ³å£°å¯¾è©±ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    # çŠ¶æ…‹å®šç¾©
    STATE_IDLE = "idle"
    STATE_LISTENING = "listening"
    STATE_PROCESSING = "processing"
    STATE_SPEAKING = "speaking"

    def __init__(
        self,
        chat_config: Optional[ChatConfig] = None,
        stt_model: str = "small",
        tts_models_dir: str = "models/tts/kokoro",
        tts_voice: str = "jf_alpha",
        vad_type: str = "auto",
        streaming_tts: bool = True,
        enable_rag: bool = True,
        enable_vision: bool = True,
        camera_id: int = 0,
    ):
        # ãƒãƒ£ãƒƒãƒˆè¨­å®š
        self.config = chat_config or ChatConfig.load(PROJECT_ROOT / "config" / "chat_config.json")

        # STT (faster-whisper)
        self.stt = WhisperSTT(
            model_size=stt_model,
            language="ja",
            device="cpu",
            compute_type="int8",
        )

        # TTS (kokoro-onnx)
        self.tts = KokoroTTS(
            models_dir=PROJECT_ROOT / tts_models_dir,
            voice=tts_voice,
        )

        # VAD (auto: Sileroå„ªå…ˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§Energy)
        self.vad_type = vad_type
        self.vad = create_vad(vad_type=vad_type, sample_rate=16000)

        # ã‚ªãƒ¼ãƒ‡ã‚£ã‚ª I/O
        self.recorder = AudioRecorder(sample_rate=16000)
        self.player = AudioPlayer(sample_rate=24000)

        # LLM
        self.llm = OllamaClient(
            base_url=self.config.ollama_base_url,
            model=self.config.model,
        )

        # RAG (Phase 4: é•·æœŸè¨˜æ†¶)
        self.enable_rag = enable_rag
        self.rag = None
        if enable_rag:
            try:
                self.vector_store = VectorStore(
                    persist_dir=str(PROJECT_ROOT / "data" / "vectordb"),
                )
                self.rag = RAGRetriever(vector_store=self.vector_store)
            except Exception as e:
                print(f"âš ï¸  RAGåˆæœŸåŒ–ã‚¹ã‚­ãƒƒãƒ—: {e}")
                self.rag = None

        # Vision (Phase 5: æ˜ åƒå…¥åŠ›)
        self.enable_vision = enable_vision
        self.vision_context: Optional[VisionContext] = None
        if enable_vision:
            try:
                emotion_model = str(PROJECT_ROOT / "models" / "vision" / "emotion-ferplus-8.onnx")
                self.vision_context = VisionContext(
                    camera_id=camera_id,
                    analysis_interval=2.0,
                    emotion_model_path=emotion_model,
                )
            except Exception as e:
                print(f"âš ï¸  VisionåˆæœŸåŒ–ã‚¹ã‚­ãƒƒãƒ—: {e}")
                self.vision_context = None

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³
        self.session = ChatSession(
            system_prompt=self.config.system_prompt,
            max_history_turns=self.config.max_history_turns,
            history_dir=str(PROJECT_ROOT / self.config.history_dir),
            rag=self.rag,
            vision_context=self.vision_context,
        )

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°TTSè¨­å®š
        self.streaming_tts = streaming_tts
        self._tts_queue: queue.Queue = queue.Queue()

        # çŠ¶æ…‹
        self._state = self.STATE_IDLE
        self._running = False
        self._audio_queue: queue.Queue = queue.Queue()

    # --- æ–‡åˆ†å‰²ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ---
    # æ—¥æœ¬èªã®æ–‡æœ«ãƒ‘ã‚¿ãƒ¼ãƒ³: ã€‚ï¼ï¼Ÿ!? + æ”¹è¡Œ
    _SENTENCE_SPLIT_RE = re.compile(r'(?<=[ã€‚ï¼ï¼Ÿ!?\n])')

    @staticmethod
    def _split_sentences(text: str) -> list[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡å˜ä½ã«åˆ†å‰²ã™ã‚‹"""
        parts = VoicePipeline._SENTENCE_SPLIT_RE.split(text)
        return [p for p in parts if p.strip()]

    @property
    def state(self) -> str:
        return self._state

    def initialize(self) -> bool:
        """
        å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–ã€‚èµ·å‹•æ™‚ã«1å›å‘¼ã¶ã€‚

        Returns:
            æˆåŠŸã—ãŸã‚‰ True
        """
        print("=" * 50)
        print(" éŸ³å£°å¯¾è©±ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ åˆæœŸåŒ–")
        print("=" * 50)

        # Ollama æ¥ç¶šãƒã‚§ãƒƒã‚¯
        print("\n[1/4] Ollama æ¥ç¶šç¢ºèª...")
        if not self.llm.is_available():
            print("âŒ Ollamaã«æ¥ç¶šã§ãã¾ã›ã‚“")
            return False
        print("âœ… Ollama OK")

        # STT ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        print("\n[2/4] STT ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰...")
        try:
            self.stt.load()
            print("âœ… STT OK")
        except Exception as e:
            print(f"âŒ STT ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            return False

        # TTS ãƒã‚§ãƒƒã‚¯
        print("\n[3/4] TTS ç¢ºèª...")
        try:
            self.tts.load()
            print("âœ… TTS OK")
        except Exception as e:
            print(f"âŒ TTS ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            return False

        # VAD ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ (Energy VADã®å ´åˆã®ã¿ç’°å¢ƒãƒã‚¤ã‚ºè¨ˆæ¸¬)
        print("\n[4/4] VAD ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³...")
        vad_name = type(self.vad).__name__
        print(f"  VADæ–¹å¼: {vad_name}")
        try:
            if isinstance(self.vad, EnergyVAD):
                print("  ç’°å¢ƒãƒã‚¤ã‚ºã‚’è¨ˆæ¸¬ä¸­ (2ç§’é–“ã€é™ã‹ã«ã—ã¦ãã ã•ã„)...")
                noise_sample = self.recorder.record(2.0)
                self.vad.calibrate(noise_sample)
            else:
                self.vad.calibrate(np.zeros(16000, dtype=np.float32))
            print("âœ… VAD OK")
        except Exception as e:
            print(f"âš ï¸  VAD ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•— (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤ã‚’ä½¿ç”¨): {e}")

        # RAG (Phase 4)
        if self.enable_rag and self.rag is not None:
            print("\n[5/6] RAG (é•·æœŸè¨˜æ†¶) åˆæœŸåŒ–...")
            try:
                self.vector_store.initialize()
                stats = self.rag.get_stats()
                print(f"âœ… RAG OK (ä¼šè©±: {stats['conversations']}ä»¶, çŸ¥è­˜: {stats['knowledge']}ä»¶)")
            except Exception as e:
                print(f"âš ï¸  RAG åˆæœŸåŒ–å¤±æ•— (RAGãªã—ã§ç¶šè¡Œ): {e}")
                self.session.rag = None
        else:
            print("\n[5/6] RAG (é•·æœŸè¨˜æ†¶) ã‚¹ã‚­ãƒƒãƒ—")

        # Vision (Phase 5)
        if self.enable_vision and self.vision_context is not None:
            print("\n[6/6] Vision (æ˜ åƒå…¥åŠ›) åˆæœŸåŒ–...")
            try:
                if self.vision_context.start():
                    import time
                    time.sleep(1.0)  # ã‚«ãƒ¡ãƒ©å®‰å®šå¾…ã¡
                    status = self.vision_context.get_status()
                    emotion_str = "æœ‰åŠ¹" if status["emotion_detection"] else "é¡”æ¤œå‡ºã®ã¿"
                    print(f"âœ… Vision OK (ã‚«ãƒ¡ãƒ©èµ·å‹•, æ„Ÿæƒ…æ¨å®š: {emotion_str})")
                else:
                    print("âš ï¸  ã‚«ãƒ¡ãƒ©ã‚’é–‹ã‘ã¾ã›ã‚“ (Visionãªã—ã§ç¶šè¡Œ)")
                    self.session.vision_context = None
                    self.vision_context = None
            except Exception as e:
                print(f"âš ï¸  Vision åˆæœŸåŒ–å¤±æ•— (Visionãªã—ã§ç¶šè¡Œ): {e}")
                self.session.vision_context = None
                self.vision_context = None
        else:
            print("\n[6/6] Vision (æ˜ åƒå…¥åŠ›) ã‚¹ã‚­ãƒƒãƒ—")

        print("\n" + "=" * 50)
        print(" âœ… åˆæœŸåŒ–å®Œäº†ï¼")
        print("=" * 50)
        return True

    def process_voice_turn(self) -> Optional[str]:
        """
        1ã‚¿ãƒ¼ãƒ³ã®éŸ³å£°å¯¾è©±ã‚’å‡¦ç†ã™ã‚‹:
        éŒ²éŸ³ â†’ STT â†’ LLM â†’ TTS â†’ å†ç”Ÿ

        streaming_tts=True ã®å ´åˆã€LLMã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’æ–‡å˜ä½ã§
        é€æ¬¡TTSåˆæˆãƒ»å†ç”Ÿã™ã‚‹ï¼ˆå…¨æ–‡å®Œæˆã‚’å¾…ãŸãªã„ï¼‰ã€‚

        Returns:
            AIã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã€‚ã‚¨ãƒ©ãƒ¼æ™‚ã¯ Noneã€‚
        """
        # --- ãƒªã‚¹ãƒ‹ãƒ³ã‚° ---
        self._state = self.STATE_LISTENING
        print("\nğŸ¤ èã„ã¦ã„ã¾ã™... (è©±ã—çµ‚ã‚ã£ãŸã‚‰è‡ªå‹•æ¤œå‡ºã—ã¾ã™)")

        speech_audio = self._listen_for_speech()
        if speech_audio is None or len(speech_audio) < self.vad.sample_rate * 0.3:
            return None

        # --- STT ---
        self._state = self.STATE_PROCESSING
        print("\nğŸ”„ éŸ³å£°èªè­˜ä¸­...")
        user_text = self.stt.transcribe(speech_audio)

        if not user_text:
            print("  (éŸ³å£°ã‚’èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ)")
            return None

        print(f"\nğŸ‘¤ ã‚ãªãŸ: {user_text}")

        # --- LLM â†’ TTS (ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°) ---
        print("\nğŸ¤– è€ƒãˆä¸­...")
        self.session.add_user_message(user_text)
        messages = self.session.build_messages()

        try:
            if self.streaming_tts:
                response_text = self._stream_llm_with_tts(messages)
            else:
                response_text = self._sequential_llm_then_tts(messages)

            if not response_text:
                return None

            self.session.add_assistant_message(response_text)

        except Exception as e:
            print(f"\nâŒ LLM ã‚¨ãƒ©ãƒ¼: {e}")
            if self.session._messages and self.session._messages[-1]["role"] == "user":
                self.session._messages.pop()
            return None

        self._state = self.STATE_IDLE
        return response_text

    def _stream_llm_with_tts(self, messages: list[dict]) -> str:
        """
        LLMã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’æ–‡å˜ä½ã§TTSåˆæˆãƒ»å†ç”Ÿã™ã‚‹

        LLMãŒãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹é–“ã€æ–‡ã®åŒºåˆ‡ã‚Šã‚’æ¤œå‡ºã—ã¦
        å®Œæˆã—ãŸæ–‡ã‹ã‚‰é †ã«TTSã‚­ãƒ¥ãƒ¼ã«æŠ•å…¥ãƒ»å†ç”Ÿã™ã‚‹ã€‚
        """
        response_text = ""
        sentence_buffer = ""
        tts_thread = None
        played_sentences: list[str] = []

        # TTSå†ç”Ÿãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰
        self._tts_stop = False
        tts_thread = threading.Thread(target=self._tts_worker, daemon=True)
        tts_thread.start()

        self._state = self.STATE_PROCESSING
        try:
            for token in self.llm.generate_stream(
                messages,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                top_k=self.config.top_k,
                num_ctx=self.config.num_ctx,
                repeat_penalty=self.config.repeat_penalty,
            ):
                response_text += token
                sentence_buffer += token
                print(token, end="", flush=True)

                # æ–‡ã®åŒºåˆ‡ã‚Šã‚’ãƒã‚§ãƒƒã‚¯
                sentences = self._split_sentences(sentence_buffer)
                if len(sentences) > 1:
                    # æœ€å¾Œã®è¦ç´ ã¯ã¾ã ä¸å®Œå…¨ãªå¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ä¿æŒ
                    for sent in sentences[:-1]:
                        sent = sent.strip()
                        if sent:
                            self._tts_queue.put(sent)
                            played_sentences.append(sent)
                    sentence_buffer = sentences[-1]

            # æ®‹ã‚Šã®ãƒãƒƒãƒ•ã‚¡ã‚‚é€ä¿¡
            if sentence_buffer.strip():
                self._tts_queue.put(sentence_buffer.strip())
                played_sentences.append(sentence_buffer.strip())

            print()  # æ”¹è¡Œ

        finally:
            # TTSå®Œäº†ã‚’å¾…æ©Ÿ
            self._tts_queue.put(None)  # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
            if tts_thread:
                tts_thread.join(timeout=60)

        return response_text

    def _tts_worker(self) -> None:
        """TTSåˆæˆãƒ»å†ç”Ÿã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰"""
        while True:
            text = self._tts_queue.get()
            if text is None:
                break
            if self._tts_stop:
                break

            self._state = self.STATE_SPEAKING
            try:
                wav_data = self.tts.synthesize(text)
                self.player.play_wav(wav_data, blocking=True)
            except Exception as e:
                print(f"\nâš ï¸  TTSå†ç”Ÿã‚¨ãƒ©ãƒ¼: {e}")

    def _sequential_llm_then_tts(self, messages: list[dict]) -> str:
        """å¾“æ¥ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«æ–¹å¼: LLMå…¨æ–‡å®Œäº†å¾Œã«TTS"""
        response_text = ""
        for token in self.llm.generate_stream(
            messages,
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            top_k=self.config.top_k,
            num_ctx=self.config.num_ctx,
            repeat_penalty=self.config.repeat_penalty,
        ):
            response_text += token
            print(token, end="", flush=True)
        print()

        if not response_text:
            return ""

        # --- TTS & å†ç”Ÿ ---
        self._state = self.STATE_SPEAKING
        print("\nğŸ”Š èª­ã¿ä¸Šã’ä¸­...")
        try:
            wav_data = self.tts.synthesize(response_text)
            self.player.play_wav(wav_data, blocking=True)
        except Exception as e:
            print(f"âš ï¸  TTS/å†ç”Ÿã‚¨ãƒ©ãƒ¼: {e}")

        return response_text

    def _listen_for_speech(self, max_duration: float = 30.0) -> Optional[np.ndarray]:
        """
        VADã‚’ä½¿ã£ã¦ç™ºè©±åŒºé–“ã‚’æ¤œå‡ºã—ã€éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™

        Args:
            max_duration: æœ€å¤§éŒ²éŸ³æ™‚é–“ (ç§’)

        Returns:
            æ¤œå‡ºã•ã‚ŒãŸç™ºè©±ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã€‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã¯ Noneã€‚
        """
        self.vad.reset()
        result_audio = None
        start_time = time.time()

        def audio_callback(indata, frames, time_info, status):
            nonlocal result_audio
            if status:
                pass  # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ç­‰ã¯ç„¡è¦–
            frame = indata[:, 0].copy()  # ãƒ¢ãƒãƒ©ãƒ«ã«
            speech = self.vad.process_frame(frame)
            if speech is not None:
                self._audio_queue.put(speech)

        stream = self.recorder.open_stream(
            callback=audio_callback,
            frame_size=self.vad.frame_size,
        )

        with stream:
            while True:
                try:
                    # ç™ºè©±æ¤œå‡ºå¾…ã¡
                    speech = self._audio_queue.get(timeout=1.0)
                    return speech
                except queue.Empty:
                    elapsed = time.time() - start_time
                    if elapsed > max_duration:
                        print("  (ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ)")
                        return None
                    if self.vad.is_speaking:
                        # ç™ºè©±ä¸­ã®ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿
                        print(".", end="", flush=True)

    def run_interactive(self) -> None:
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–éŸ³å£°å¯¾è©±ãƒ«ãƒ¼ãƒ—"""
        self._running = True
        print("\n" + "=" * 50)
        print(" ğŸ™ï¸  éŸ³å£°å¯¾è©±ãƒ¢ãƒ¼ãƒ‰")
        print("  Ctrl+C ã§çµ‚äº†")
        print("=" * 50)

        try:
            while self._running:
                self.process_voice_turn()
        except KeyboardInterrupt:
            print("\n\nçµ‚äº†ã—ã¾ã™...")
            self._running = False
            if self.session.turn_count > 0:
                saved = self.session.save()
                print(f"ä¼šè©±ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {saved}")
            self.llm.close()

    def cleanup(self) -> None:
        """ãƒªã‚½ãƒ¼ã‚¹ã®è§£æ”¾"""
        self._running = False
        if self.vision_context is not None:
            self.vision_context.stop()
        self.llm.close()
