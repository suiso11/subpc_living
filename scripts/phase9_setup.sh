#!/bin/bash
# ===================================================
# Phase 9 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# GPUæ›è£…æº–å‚™ â€” ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å´ã®è¨­å®šç¢ºèªãƒ»æ›´æ–°
# ===================================================
set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
VENV_DIR="${PROJECT_ROOT}/.venv"

echo "================================================="
echo " Phase 9: GPUæ›è£… â€” ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
echo "================================================="
echo ""

# --- 0. ä»®æƒ³ç’°å¢ƒã®ç¢ºèª ---
if [ ! -d "$VENV_DIR" ]; then
    echo "âŒ ä»®æƒ³ç’°å¢ƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã« phase2_setup.sh ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
    exit 1
fi
source "${VENV_DIR}/bin/activate"
echo "âœ… ä»®æƒ³ç’°å¢ƒ: ${VENV_DIR}"

# --- 1. GPU æ¤œå‡º ---
echo ""
echo "[1/4] GPU æ¤œå‡º..."
if command -v nvidia-smi &> /dev/null; then
    GPU_NAME=$(nvidia-smi --query-gpu=gpu_name --format=csv,noheader 2>/dev/null || echo "unknown")
    GPU_VRAM=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits 2>/dev/null || echo "0")
    DRIVER=$(nvidia-smi --query-gpu=driver_version --format=csv,noheader 2>/dev/null || echo "unknown")
    echo "  GPU: ${GPU_NAME}"
    echo "  VRAM: ${GPU_VRAM} MB"
    echo "  Driver: ${DRIVER}"
else
    echo "âš ï¸  nvidia-smi ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚GPU ãªã—ç’°å¢ƒã§ã™ã€‚"
    GPU_NAME="none"
    GPU_VRAM="0"
fi

# --- 2. gpu_config ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ†ã‚¹ãƒˆ ---
echo ""
echo "[2/4] GPU è¨­å®šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç¢ºèª..."
python3 -c "
import sys
sys.path.insert(0, '${PROJECT_ROOT}')
from src.service.gpu_config import get_device_config
config = get_device_config()
print(f'  Profile: {config.profile}')
print(f'  STT: device={config.stt_device}, compute={config.stt_compute_type}, model={config.stt_model_size}')
print(f'  Embedding: device={config.embedding_device}')
print(f'  ONNX providers: {config.onnx_providers}')
print(f'  æ¨å¥¨LLMãƒ¢ãƒ‡ãƒ«: {config.recommended_model}')
"
echo "âœ… gpu_config OK"

# --- 3. onnxruntime ç¢ºèª ---
echo ""
echo "[3/4] ONNX Runtime ç¢ºèª..."
python3 -c "
import onnxruntime as ort
providers = ort.get_available_providers()
print(f'  åˆ©ç”¨å¯èƒ½ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {providers}')
if 'CUDAExecutionProvider' in providers:
    print('  âœ… CUDAExecutionProvider åˆ©ç”¨å¯èƒ½')
else:
    print('  â„¹ï¸  CPUExecutionProvider ã®ã¿ (GPU ONNX ãŒå¿…è¦ãªå ´åˆ: pip install onnxruntime-gpu)')
"

# --- 4. P40 æ›è£…ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ ---
echo ""
echo "[4/4] P40 æ›è£…ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ..."
if echo "${GPU_VRAM}" | grep -qE '^[2-9][0-9]{4}'; then
    echo "  âœ… å¤§å®¹é‡VRAM GPU æ¤œå‡º â€” P40ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™"
    echo ""
    echo "  ğŸ“‹ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:"
    echo "    1. LLMãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰:"
    echo "       ollama pull qwen2.5:14b-instruct-q4_K_M"
    echo "    2. config/chat_config.json ã® model ã‚’å¤‰æ›´:"
    echo '       "model": "qwen2.5:14b-instruct-q4_K_M"'
    echo "    3. STT medium ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (åˆå›è‡ªå‹•):"
    echo "       python3 -c \"from faster_whisper import WhisperModel; WhisperModel('medium', device='cuda')\""
else
    echo "  â„¹ï¸  ç¾åœ¨ GTX 1060 ç›¸å½“ã®ç’°å¢ƒã§ã™ã€‚"
    echo "  P40 æ›è£…å¾Œã«å†åº¦ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
    echo ""
    echo "  ğŸ“‹ P40 æ›è£…æ‰‹é †:"
    echo "    1. P40 ã‚’ç‰©ç†çš„ã«å–ã‚Šä»˜ã‘"
    echo "    2. é›»æº 500Wâ†’650W ã¸ã®æ›è£…ã‚’æ¨å¥¨ (P40 TDP: 250W)"
    echo "    3. BIOS ã§ iGPU ã‚’æ˜ åƒå‡ºåŠ›ã«è¨­å®š (P40 ã¯æ˜ åƒå‡ºåŠ›ãªã—)"
    echo "    4. Ubuntu èµ·å‹•å¾Œ nvidia-smi ã§èªè­˜ç¢ºèª"
    echo "    5. bash scripts/phase9_setup.sh ã‚’å†å®Ÿè¡Œ"
    echo "    6. config/chat_config.json ã® model ã‚’ 14b ã«å¤‰æ›´"
fi

echo ""
echo "================================================="
echo " âœ… Phase 9 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†"
echo "================================================="
